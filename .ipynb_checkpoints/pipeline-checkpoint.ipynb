{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86fa5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import networkx as nx\n",
    "import wikipedia\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37834b05",
   "metadata": {},
   "source": [
    "### Classe Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6533fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        idx = 0\n",
    "        if depends_on:\n",
    "            idx = self.tasks.index(depends_on) + 1\n",
    "        def inner(f):\n",
    "            self.tasks.insert(idx, f)\n",
    "            return f\n",
    "        return inner\n",
    "    \n",
    "    def run(self, input_):\n",
    "        output = input_\n",
    "        for task in self.tasks:\n",
    "            output = task(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f703a",
   "metadata": {},
   "source": [
    "Instanciando um objeto da classe pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d39edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7fcc7c",
   "metadata": {},
   "source": [
    "Define o primeiro nó do pipeline, a função responsável por gerar o grafo a partir de uma página wikipedia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7063304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task()\n",
    "def fetch_wikipedia_graph(seed, num_layers=2, debug=True):\n",
    "    \"\"\"\n",
    "    Fetches Wikipedia page data and captures the nodes and edges up to a specified number of layers.\n",
    "\n",
    "    Args:\n",
    "        seed (str): The seed page title to start the traversal.\n",
    "        num_layers (int, optional): The number of layers to traverse from the seed page. Defaults to 2.\n",
    "        debug (bool, optional): Flag to enable debug mode for displaying progress information. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        networkx.DiGraph: A directed graph representing the nodes and edges captured from Wikipedia.\n",
    "\n",
    "    Raises:\n",
    "        WikipediaException: If there's an error loading a page.\n",
    "\n",
    "    \"\"\"\n",
    "    print('Capturing data')\n",
    "    seed = seed.title()\n",
    "    stops = (\n",
    "        \"International Standard Serial Number\",\n",
    "        \"International Standard Book Number\",\n",
    "        \"National Diet Library\",\n",
    "        \"International Standard Name Identifier\",\n",
    "        \"International Standard Book Number (Identifier)\",\n",
    "        \"Pubmed Identifier\",\n",
    "        \"Pubmed Central\",\n",
    "        \"Digital Object Identifier\",\n",
    "        \"Arxiv\",\n",
    "        \"Proc Natl Acad Sci Usa\",\n",
    "        \"Bibcode\",\n",
    "        \"Library Of Congress Control Number\",\n",
    "        \"Jstor\",\n",
    "        \"Doi (Identifier)\",\n",
    "        \"Isbn (Identifier)\",\n",
    "        \"Pmid (Identifier)\",\n",
    "        \"Arxiv (Identifier)\",\n",
    "        \"Bibcode (Identifier)\"\n",
    "    )\n",
    "    todo_lst = [(0, seed)]\n",
    "    todo_set = set(seed)\n",
    "    done_set = set()\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    while todo_lst and todo_lst[0][0] < num_layers:\n",
    "        layer, page = todo_lst.pop(0)\n",
    "        done_set.add(page)\n",
    "\n",
    "        # Show progress\n",
    "        if debug:\n",
    "            print(layer, page)\n",
    "\n",
    "        try:\n",
    "            wiki = wikipedia.page(page)\n",
    "        except:\n",
    "            if debug:\n",
    "                print(\"Could not load\", page)\n",
    "            continue\n",
    "\n",
    "        for link in wiki.links:\n",
    "            link = link.title()\n",
    "            if link not in stops and not link.startswith(\"List Of\"):\n",
    "                if link not in todo_set and link not in done_set:\n",
    "                    todo_lst.append((layer + 1, link))\n",
    "                    todo_set.add(link)\n",
    "                graph.add_edge(page, link)\n",
    "\n",
    "    print('Data capture finished')\n",
    "    print(\"{} nodes, {} edges\".format(len(graph), nx.number_of_edges(graph)))\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7e7ac6",
   "metadata": {},
   "source": [
    "Define o próximo nó do pipeline, a função responsável por remover os nós duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc81bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=fetch_wikipedia_graph)\n",
    "def remove_duplicate_nodes(graph):\n",
    "    \"\"\"\n",
    "    Removes duplicate nodes and edges from the graph.\n",
    "\n",
    "    Args:\n",
    "        graph (networkx.DiGraph): The graph representing Wikipedia nodes and edges.\n",
    "\n",
    "    Returns:\n",
    "        networkx.DiGraph: The graph with duplicate nodes and edges removed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    graph.remove_edges_from(nx.selfloop_edges(graph))\n",
    "\n",
    "    duplicates = [(node, node + \"s\")\n",
    "                  for node in graph if node + \"s\" in graph\n",
    "                 ]\n",
    "\n",
    "    for dup in duplicates:\n",
    "        graph = nx.contracted_nodes(graph, *dup, self_loops=False)\n",
    "\n",
    "    print(duplicates)\n",
    "\n",
    "    duplicates = [(x, y) for x, y in\n",
    "                  [(node, node.replace(\"-\", \" \")) for node in graph]\n",
    "                  if x != y and y in graph]\n",
    "    print(duplicates)\n",
    "\n",
    "    for dup in duplicates:\n",
    "        graph = nx.contracted_nodes(graph, *dup, self_loops=False)\n",
    "\n",
    "    nx.set_node_attributes(graph, 0, \"contraction\")\n",
    "    nx.set_edge_attributes(graph, 0, \"contraction\")\n",
    "    print('Duplicates removed')\n",
    "    print(\"{} nodes, {} edges\".format(len(graph), nx.number_of_edges(graph)))\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9b3d6a",
   "metadata": {},
   "source": [
    "Define o próximo nó do pipeline, a função responsável por remover os nós duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c23786d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=remove_duplicate_nodes)\n",
    "def filter_high_degree_nodes(graph, degree=2):\n",
    "    \"\"\"\n",
    "    Filters nodes in the graph based on their degree.\n",
    "\n",
    "    Args:\n",
    "        graph (networkx.DiGraph): The graph representing Wikipedia nodes and edges.\n",
    "        degree (int, optional): The minimum degree threshold for filtering nodes. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        networkx.DiGraph: The subgraph containing nodes with degree greater than the specified threshold.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    core = [node for node, drg in dict(graph.degree()).items() if drg > degree]\n",
    "    subgraph = graph.subgraph(core)\n",
    "    print('Filter')\n",
    "    print(\"{} nodes, {} edges\".format(len(subgraph), nx.number_of_edges(subgraph)))\n",
    "\n",
    "    return subgraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8d8fae",
   "metadata": {},
   "source": [
    "Nó do pipeline responsável pelo plot dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ded00972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from plot_metrics import (\n",
    "    plot_centrality,\n",
    "    plot_degree_centrality,\n",
    "    plot_closeness_centrality,\n",
    "    plot_betweenness_centrality,\n",
    "    plot_eigenvector_centrality,\n",
    "    plot_pdf,\n",
    ")\n",
    "\n",
    "@pipeline.task(depends_on=filter_high_degree_nodes)\n",
    "def plot_results(subgraph):\n",
    "    \"\"\"\n",
    "    Shows the results of the analysis by plotting various centrality measures and creating visualizations.\n",
    "\n",
    "    Args:\n",
    "        subgraph (networkx.DiGraph): The subgraph containing filtered nodes based on degree.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    plot_pdf(subgraph)\n",
    "    pos = nx.spring_layout(subgraph, seed=8375, k=0.2)\n",
    "\n",
    "    plot_degree_centrality(subgraph, pos)\n",
    "    plot_closeness_centrality(subgraph, pos)\n",
    "    plot_betweenness_centrality(subgraph, pos)\n",
    "    plot_eigenvector_centrality(subgraph, pos)\n",
    "\n",
    "    bc = pd.Series(nx.betweenness_centrality(subgraph))\n",
    "    dc = pd.Series(nx.degree_centrality(subgraph))\n",
    "    ec = pd.Series(nx.eigenvector_centrality(subgraph))\n",
    "    cc = pd.Series(nx.closeness_centrality(subgraph))\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\"Betweenness\": bc,\n",
    "                                 \"Degree\": dc,\n",
    "                                 \"EigenVector\": ec,\n",
    "                                 \"Closeness\": cc})\n",
    "    print('Plot all')\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    fig = sns.PairGrid(df)\n",
    "    fig.map_upper(sns.scatterplot)\n",
    "    fig.map_lower(sns.kdeplot, cmap=\"Reds_r\")\n",
    "    fig.map_diag(sns.kdeplot, lw=2, legend=False)\n",
    "\n",
    "    plt.savefig('all.png', transparent=True, dpi=800, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print('Plot Core and shell')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "    g2_core_9 = nx.k_shell(subgraph, 9)\n",
    "    g2_core_10 = nx.k_core(subgraph, 10)\n",
    "\n",
    "    pos = nx.spring_layout(subgraph, seed=123456789, k=0.3)\n",
    "\n",
    "    nx.draw_networkx_edges(subgraph, pos=pos, alpha=0.4, ax=ax)\n",
    "\n",
    "    nodes = nx.draw_networkx_nodes(subgraph, pos=pos, node_color=\"#333333\")\n",
    "    nodes = nx.draw_networkx_nodes(g2_core_9, pos=pos, node_color=\"blue\")\n",
    "    nodes = nx.draw_networkx_nodes(g2_core_10, pos=pos, node_color=\"red\")\n",
    "\n",
    "    red_patch = mpatches.Patch(color='red', label='10-core')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='9-shell')\n",
    "    plt.legend(handles=[red_patch, blue_patch])\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('k-core_sociopatterns.png', transparent=True, dpi=600)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc989e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andremeneses/Documents/envs/ml/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/andremeneses/Documents/envs/ml/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "pipeline.run(\"Gödel's incompleteness theorems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7bdf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
