{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86fa5536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "import networkx as nx\n",
    "import wikipedia\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85c03a5",
   "metadata": {},
   "source": [
    "### Classe Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56513507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self):\n",
    "        self.tasks = []\n",
    "        \n",
    "    def task(self, depends_on=None):\n",
    "        idx = 0\n",
    "        if depends_on:\n",
    "            idx = self.tasks.index(depends_on) + 1\n",
    "        def inner(f):\n",
    "            self.tasks.insert(idx, f)\n",
    "            return f\n",
    "        return inner\n",
    "    \n",
    "    def run(self, input_):\n",
    "        output = input_\n",
    "        for task in self.tasks:\n",
    "            output = task(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fc5f62",
   "metadata": {},
   "source": [
    "Instanciando um objeto da classe pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2ba2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46813df9",
   "metadata": {},
   "source": [
    "Define o primeiro nó do pipeline, a função responsável por gerar o grafo a partir de uma página wikipedia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a25242",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task()\n",
    "def fetch_wikipedia_graph(seed, num_layers=2, debug=True):\n",
    "    \"\"\"\n",
    "    Fetches Wikipedia page data and captures the nodes and edges up to a specified number of layers.\n",
    "\n",
    "    Args:\n",
    "        seed (str): The seed page title to start the traversal.\n",
    "        num_layers (int, optional): The number of layers to traverse from the seed page. Defaults to 2.\n",
    "        debug (bool, optional): Flag to enable debug mode for displaying progress information. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        networkx.DiGraph: A directed graph representing the nodes and edges captured from Wikipedia.\n",
    "\n",
    "    Raises:\n",
    "        WikipediaException: If there's an error loading a page.\n",
    "\n",
    "    \"\"\"\n",
    "    print('Capturing data')\n",
    "    seed = seed.title()\n",
    "    stops = (\n",
    "        \"International Standard Serial Number\",\n",
    "        \"International Standard Book Number\",\n",
    "        \"National Diet Library\",\n",
    "        \"International Standard Name Identifier\",\n",
    "        \"International Standard Book Number (Identifier)\",\n",
    "        \"Pubmed Identifier\",\n",
    "        \"Pubmed Central\",\n",
    "        \"Digital Object Identifier\",\n",
    "        \"Arxiv\",\n",
    "        \"Proc Natl Acad Sci Usa\",\n",
    "        \"Bibcode\",\n",
    "        \"Library Of Congress Control Number\",\n",
    "        \"Jstor\",\n",
    "        \"Doi (Identifier)\",\n",
    "        \"Isbn (Identifier)\",\n",
    "        \"Pmid (Identifier)\",\n",
    "        \"Arxiv (Identifier)\",\n",
    "        \"Bibcode (Identifier)\"\n",
    "    )\n",
    "    todo_lst = [(0, seed)]\n",
    "    todo_set = set(seed)\n",
    "    done_set = set()\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    while todo_lst and todo_lst[0][0] < num_layers:\n",
    "        layer, page = todo_lst.pop(0)\n",
    "        done_set.add(page)\n",
    "\n",
    "        # Show progress\n",
    "        if debug:\n",
    "            print(layer, page)\n",
    "\n",
    "        try:\n",
    "            wiki = wikipedia.page(page)\n",
    "        except:\n",
    "            if debug:\n",
    "                print(\"Could not load\", page)\n",
    "            continue\n",
    "\n",
    "        for link in wiki.links:\n",
    "            link = link.title()\n",
    "            if link not in stops and not link.startswith(\"List Of\"):\n",
    "                if link not in todo_set and link not in done_set:\n",
    "                    todo_lst.append((layer + 1, link))\n",
    "                    todo_set.add(link)\n",
    "                graph.add_edge(page, link)\n",
    "\n",
    "    print('Data capture finished')\n",
    "    print(\"{} nodes, {} edges\".format(len(graph), nx.number_of_edges(graph)))\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0940215d",
   "metadata": {},
   "source": [
    "Define o próximo nó do pipeline, a função responsável por remover os nós duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a157911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=fetch_wikipedia_graph)\n",
    "def remove_duplicate_nodes(graph):\n",
    "    \"\"\"\n",
    "    Removes duplicate nodes and edges from the graph.\n",
    "\n",
    "    Args:\n",
    "        graph (networkx.DiGraph): The graph representing Wikipedia nodes and edges.\n",
    "\n",
    "    Returns:\n",
    "        networkx.DiGraph: The graph with duplicate nodes and edges removed.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    graph.remove_edges_from(nx.selfloop_edges(graph))\n",
    "\n",
    "    duplicates = [(node, node + \"s\")\n",
    "                  for node in graph if node + \"s\" in graph\n",
    "                 ]\n",
    "\n",
    "    for dup in duplicates:\n",
    "        graph = nx.contracted_nodes(graph, *dup, self_loops=False)\n",
    "\n",
    "    print(duplicates)\n",
    "\n",
    "    duplicates = [(x, y) for x, y in\n",
    "                  [(node, node.replace(\"-\", \" \")) for node in graph]\n",
    "                  if x != y and y in graph]\n",
    "    print(duplicates)\n",
    "\n",
    "    for dup in duplicates:\n",
    "        graph = nx.contracted_nodes(graph, *dup, self_loops=False)\n",
    "\n",
    "    nx.set_node_attributes(graph, 0, \"contraction\")\n",
    "    nx.set_edge_attributes(graph, 0, \"contraction\")\n",
    "    print('Duplicates removed')\n",
    "    print(\"{} nodes, {} edges\".format(len(graph), nx.number_of_edges(graph)))\n",
    "\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba51393",
   "metadata": {},
   "source": [
    "Define o próximo nó do pipeline, a função responsável por remover os nós duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cff91f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=remove_duplicate_nodes)\n",
    "def filter_high_degree_nodes(graph, degree=2):\n",
    "    \"\"\"\n",
    "    Filters nodes in the graph based on their degree.\n",
    "\n",
    "    Args:\n",
    "        graph (networkx.DiGraph): The graph representing Wikipedia nodes and edges.\n",
    "        degree (int, optional): The minimum degree threshold for filtering nodes. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        networkx.DiGraph: The subgraph containing nodes with degree greater than the specified threshold.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    core = [node for node, drg in dict(graph.degree()).items() if drg > degree]\n",
    "    subgraph = graph.subgraph(core)\n",
    "    print('Filter')\n",
    "    print(\"{} nodes, {} edges\".format(len(subgraph), nx.number_of_edges(subgraph)))\n",
    "    nx.write_graphml(subgraph, \"cna.graphml\")\n",
    "    \n",
    "    return subgraph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463f3bd",
   "metadata": {},
   "source": [
    "Nó do pipeline responsável pelo plot dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace095c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from plot_metrics import (\n",
    "    plot_centrality,\n",
    "    plot_degree_centrality,\n",
    "    plot_closeness_centrality,\n",
    "    plot_betweenness_centrality,\n",
    "    plot_eigenvector_centrality,\n",
    "    plot_degree_and_centrality,\n",
    ")\n",
    "\n",
    "@pipeline.task(depends_on=filter_high_degree_nodes)\n",
    "def plot_results(subgraph):\n",
    "    \"\"\"\n",
    "    Shows the results of the analysis by plotting various centrality measures and creating visualizations.\n",
    "\n",
    "    Args:\n",
    "        subgraph (networkx.DiGraph): The subgraph containing filtered nodes based on degree.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    pos = nx.spring_layout(subgraph, seed=8375, k=0.2)\n",
    "\n",
    "    plot_degree_centrality(subgraph)\n",
    "    plot_closeness_centrality(subgraph)\n",
    "    plot_betweenness_centrality(subgraph)\n",
    "    plot_eigenvector_centrality(subgraph)\n",
    "\n",
    "    bc = pd.Series(nx.betweenness_centrality(subgraph))\n",
    "    dc = pd.Series(nx.degree_centrality(subgraph))\n",
    "    ec = pd.Series(nx.eigenvector_centrality(subgraph))\n",
    "    cc = pd.Series(nx.closeness_centrality(subgraph))\n",
    "\n",
    "    df = pd.DataFrame.from_dict({\"Betweenness\": bc,\n",
    "                                 \"Degree\": dc,\n",
    "                                 \"EigenVector\": ec,\n",
    "                                 \"Closeness\": cc})\n",
    "    print('Plot all')\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    fig = sns.PairGrid(df)\n",
    "    fig.map_upper(sns.scatterplot)\n",
    "    fig.map_lower(sns.kdeplot, cmap=\"Reds_r\")\n",
    "    fig.map_diag(sns.kdeplot, lw=2, legend=False)\n",
    "\n",
    "    plt.savefig('all.png', transparent=True, dpi=800, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print('Plot Core and shell')\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "    g2_core_9 = nx.k_shell(subgraph, 243)\n",
    "    g2_core_10 = nx.k_core(subgraph, 394)\n",
    "\n",
    "    pos = nx.spring_layout(subgraph, seed=123456789, k=0.3)\n",
    "\n",
    "    nx.draw_networkx_edges(subgraph, pos=pos, alpha=0.4, ax=ax)\n",
    "\n",
    "    nodes = nx.draw_networkx_nodes(subgraph, pos=pos, node_color=\"#333333\")\n",
    "    nodes = nx.draw_networkx_nodes(g2_core_9, pos=pos, node_color=\"blue\")\n",
    "    nodes = nx.draw_networkx_nodes(g2_core_10, pos=pos, node_color=\"red\")\n",
    "\n",
    "    red_patch = mpatches.Patch(color='red', label='394-core')\n",
    "    blue_patch = mpatches.Patch(color='blue', label='243-shell')\n",
    "    plt.legend(handles=[red_patch, blue_patch])\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig('k-core_sociopatterns.png', transparent=True, dpi=600)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11cb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capturing data\n",
      "0 Gödel'S Incompleteness Theorems\n",
      "1 A. K. Peters\n",
      "1 Ascii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andremeneses/Documents/envs/data/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/andremeneses/Documents/envs/data/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load Ascii\n",
      "1 Abstract Logic\n",
      "1 Ackermann Set Theory\n",
      "1 Alan Sokal\n",
      "1 Alan Turing\n",
      "Could not load Alan Turing\n",
      "1 Aleph Number\n",
      "1 Alfred Tarski\n",
      "1 Algebraically Closed Field\n",
      "1 Algorithm\n",
      "1 Algorithmic Information Theory\n",
      "1 Alonzo Church\n",
      "Could not load Alonzo Church\n",
      "1 Alphabet (Formal Languages)\n",
      "1 Anti-Realist\n",
      "1 Argument\n",
      "1 Arithmetical Hierarchy\n",
      "1 Arity\n",
      "Could not load Arity\n",
      "1 Atomic Formula\n",
      "1 Atomic Sentence\n",
      "1 Automata Theory\n",
      "1 Automated Theorem Proving\n",
      "1 Avi Wigderson\n",
      "1 Axiom\n",
      "1 Axiom Of Choice\n",
      "Could not load Axiom Of Choice\n",
      "1 Axiom Schema\n",
      "1 Axiom Schema Of Specification\n",
      "1 Axiomatic System\n",
      "1 Axiomatization Of Boolean Algebras\n",
      "1 Bbc\n",
      "Could not load Bbc\n",
      "1 Banach–Tarski Paradox\n",
      "1 Berry'S Paradox\n",
      "1 Bertrand Russell\n",
      "1 Bew\n",
      "1 Bew (Disambiguation)\n",
      "Could not load Bew (Disambiguation)\n",
      "1 Bijection\n",
      "Could not load Bijection\n",
      "1 Binary Operation\n",
      "1 Bob Hale (Philosopher)\n",
      "1 Bona Fide\n",
      "1 Boolean Algebra\n",
      "1 Boolean Algebras Canonically Defined\n",
      "1 Boolean Function\n",
      "1 Cantor'S Diagonal Argument\n",
      "1 Cantor'S Paradox\n",
      "1 Cantor'S Theorem\n",
      "1 Cardinality\n",
      "1 Cartesian Product\n",
      "1 Categorical Theory\n",
      "1 Category (Mathematics)\n",
      "1 Category Of Sets\n",
      "1 Category Theory\n",
      "1 Chaitin'S Incompleteness Theorem\n",
      "1 Character Encoding\n",
      "1 Characteristic (Algebra)\n",
      "1 Church Encoding\n",
      "1 Church–Turing Thesis\n",
      "1 Class (Set Theory)\n",
      "1 Classical Logic\n",
      "1 Codomain\n",
      "1 Compactness Theorem\n",
      "1 Complement (Set Theory)\n",
      "1 Complete Theory\n",
      "1 Completeness (Logic)\n",
      "1 Completeness Theorem\n",
      "1 Computability Theory\n",
      "1 Computable Function\n",
      "1 Computable Set\n",
      "1 Computably Enumerable\n",
      "1 Computably Enumerable Set\n",
      "1 Computational Complexity Theory\n",
      "1 Concrete Category\n",
      "1 Conservative Extension\n",
      "1 Consistency\n",
      "Could not load Consistency\n",
      "1 Constructible Universe\n",
      "1 Construction Of The Real Numbers\n",
      "1 Constructive Set Theory\n",
      "1 Continuum Hypothesis\n",
      "Could not load Continuum Hypothesis\n",
      "1 Contradiction\n",
      "Could not load Contradiction\n",
      "1 Coq\n",
      "1 Countable Set\n",
      "1 Crispin Wright\n",
      "Could not load Crispin Wright\n",
      "1 Dan Willard\n",
      "1 David Hilbert\n",
      "1 Decidability (Logic)\n",
      "1 Decision Problem\n",
      "1 Deductive System\n",
      "1 Dense Linear Order\n",
      "1 Diagonal Lemma\n",
      "1 Diagram (Mathematical Logic)\n",
      "1 Dialetheia\n",
      "1 Dialetheism\n",
      "1 Domain Of A Function\n",
      "1 Douglas Hofstadter\n",
      "1 Effective Method\n",
      "1 Effective Procedure\n",
      "1 Element (Mathematics)\n",
      "1 Elementary Diagram\n",
      "1 Elementary Equivalence\n",
      "1 Elementary Function Arithmetic\n",
      "1 Empty Set\n",
      "1 Encyclopedia Of Mathematics\n",
      "1 Entscheidungsproblem\n",
      "1 Enumeration\n",
      "1 Epsilon Calculus\n",
      "1 Equiconsistency\n",
      "1 Equivalence Relation\n",
      "Could not load Equivalence Relation\n",
      "1 Ernest Nagel\n",
      "1 Ernst Zermelo\n"
     ]
    }
   ],
   "source": [
    "pipeline.run(\"Gödel's incompleteness theorems\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
